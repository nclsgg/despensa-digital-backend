# üìã Guia da Cole√ß√£o Postman - Despensa Digital LLM API

## üéØ Vis√£o Geral

Esta cole√ß√£o foi atualizada para incluir a nova funcionalidade de **sele√ß√£o de provedor LLM** nas requisi√ß√µes. Agora voc√™ pode escolher entre OpenAI, Gemini ou outros provedores diretamente nas requisi√ß√µes, sem precisar alterar o provedor ativo globalmente.

## ‚öôÔ∏è Configura√ß√£o Inicial

### Vari√°veis da Cole√ß√£o

Configure estas vari√°veis na cole√ß√£o:

| Vari√°vel | Valor Padr√£o | Descri√ß√£o |
|----------|-------------|-----------|
| `baseUrl` | `http://localhost:8080` | URL base da API |
| `jwt_token` | `your_jwt_token_here` | Token JWT obtido no login |
| `pantry_id` | `123e4567-e89b-12d3-a456-426614174000` | ID da despensa |
| `llm_provider` | `openai` | Provedor LLM padr√£o |

### Como Configurar

1. **Importe a cole√ß√£o** no Postman
2. **Configure as vari√°veis**:
   - Clique com bot√£o direito na cole√ß√£o ‚Üí "Edit"
   - V√° para aba "Variables"
   - Configure os valores necess√°rios
3. **Obtenha o JWT Token**:
   - Execute o endpoint "Authentication ‚Üí Login"
   - Copie o token da resposta
   - Cole na vari√°vel `jwt_token`

## üöÄ Funcionalidades Principais

### 1. Sele√ß√£o de Provedor por Requisi√ß√£o

#### Chat Simples com Provedor Espec√≠fico
```json
POST /api/v1/llm/chat
{
  "message": "Como fazer risoto de camar√£o?",
  "provider": "gemini",  // ou "openai"
  "context": "recipe_request"
}
```

#### Requisi√ß√£o LLM Avan√ßada com Provedor
```json
POST /api/v1/llm/process
{
  "messages": [...],
  "provider": "openai",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

### 2. Configura√ß√£o de Provedores

#### Configurar OpenAI
- Endpoint: `Provider Configuration ‚Üí Configure OpenAI Provider`
- Necess√°rio: API key da OpenAI

#### Configurar Gemini
- Endpoint: `Provider Configuration ‚Üí Configure Gemini Provider`
- Necess√°rio: API key do Google AI Studio

### 3. Testes e Compara√ß√µes

#### Comparar Respostas
Use os endpoints em "Quick Tests" para comparar respostas entre provedores:
- "Simple Chat - OpenAI" vs "Simple Chat - Gemini"
- Mesmo prompt, provedores diferentes

## üìÅ Estrutura da Cole√ß√£o

### 1. **Authentication**
- Login para obter JWT token

### 2. **Recipe Generation**
- Gerar receitas com provedor espec√≠fico
- Buscar receitas por ingredientes

### 3. **LLM Direct**
- Chat simples com sele√ß√£o de provedor
- Requisi√ß√µes LLM avan√ßadas
- Constru√ß√£o de prompts
- Status dos provedores

### 4. **Provider Configuration**
- ‚ú® **NOVA SE√á√ÉO**
- Configurar OpenAI e Gemini
- Alternar entre provedores
- Testar conectividade

### 5. **Quick Tests**
- ‚ú® **ATUALIZADA**
- Testes r√°pidos com compara√ß√£o
- OpenAI vs Gemini lado a lado

## üîß Exemplos de Uso

### Cen√°rio 1: Desenvolvimento com Gemini (Gratuito)

1. Configure o Gemini:
   ```
   Provider Configuration ‚Üí Configure Gemini Provider
   ```

2. Altere a vari√°vel `llm_provider` para `gemini`

3. Teste o chat:
   ```
   Quick Tests ‚Üí Simple Chat - Gemini
   ```

### Cen√°rio 2: Produ√ß√£o com OpenAI

1. Configure o OpenAI:
   ```
   Provider Configuration ‚Üí Configure OpenAI Provider
   ```

2. Altere a vari√°vel `llm_provider` para `openai`

3. Teste funcionalidades avan√ßadas:
   ```
   LLM Direct ‚Üí Advanced LLM Request with Provider
   ```

### Cen√°rio 3: Compara√ß√£o A/B

1. Execute o mesmo prompt nos dois provedores:
   ```
   Quick Tests ‚Üí Simple Chat - OpenAI
   Quick Tests ‚Üí Simple Chat - Gemini
   ```

2. Compare as respostas para qualidade e estilo

## üé® Dicas de Uso

### Otimiza√ß√£o de Custos
- Use **Gemini** para desenvolvimento (gratuito)
- Use **OpenAI** para produ√ß√£o quando necess√°rio
- Teste ambos para encontrar o melhor para cada caso

### Testes de Performance
- Compare tempos de resposta
- Avalie qualidade das respostas
- Teste diferentes temperaturas

### Desenvolvimento Eficiente
1. **Comece com Gemini** para prototipagem
2. **Teste com OpenAI** para valida√ß√£o
3. **Configure fallback** no frontend
4. **Monitor custos** em produ√ß√£o

## üîç Troubleshooting

### Erro: "provider n√£o est√° configurado"
**Solu√ß√£o**: Execute o endpoint de configura√ß√£o do provedor antes de usar

### Erro: "API key inv√°lida"
**Solu√ß√£o**: Verifique se as chaves foram copiadas corretamente

### Erro: 401 Unauthorized
**Solu√ß√£o**: Atualize o `jwt_token` fazendo login novamente

### Erro: "nenhum provedor ativo configurado"
**Solu√ß√£o**: Configure pelo menos um provedor ou especifique `provider` na requisi√ß√£o

## üìä Monitoramento

### Vari√°veis para Debug
- Adicione `console.log` nos scripts de teste
- Use vari√°veis para rastrear qual provedor foi usado
- Monitor tempos de resposta

### M√©tricas Importantes
- Taxa de sucesso por provedor
- Tempo m√©dio de resposta
- Qualidade das respostas
- Custos por requisi√ß√£o

## üÜï O que Mudou

### ‚ú® Novidades
- ‚úÖ Campo `provider` em requisi√ß√µes de chat
- ‚úÖ Campo `provider` em gera√ß√£o de receitas
- ‚úÖ Se√ß√£o "Provider Configuration" completa
- ‚úÖ Testes comparativos entre provedores
- ‚úÖ Vari√°vel `llm_provider` para facilitar testes

### üîÑ Atualiza√ß√µes
- ‚úÖ Todos os endpoints de chat agora suportam sele√ß√£o de provedor
- ‚úÖ Gera√ß√£o de receitas com provedor espec√≠fico
- ‚úÖ Testes r√°pidos divididos por provedor

### üìà Benef√≠cios
- **Flexibilidade**: Escolha o provedor por requisi√ß√£o
- **Economia**: Use gratuito para desenvolvimento
- **Qualidade**: Compare e escolha a melhor resposta
- **Confiabilidade**: Implemente fallback entre provedores

---

**Pronto para usar!** üöÄ A cole√ß√£o est√° completamente atualizada com a nova funcionalidade de sele√ß√£o de provedores LLM.